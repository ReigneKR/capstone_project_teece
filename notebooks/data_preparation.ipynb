{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0730182f-5647-45f9-8e38-985fd8720e21",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80625eb7-f3ae-4442-8452-f4063a02ed14",
   "metadata": {},
   "source": [
    "In this section, we prepared the dataset using a pipeline. We used different scaling techniques such as standard scaler to prepare the data for training. Additionally, we also used one-hot encoding to encode categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e38194-b0e5-4a27-83cb-6fcc0adfa55b",
   "metadata": {},
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3454e4b7-d029-4d06-8e91-8bfc1c262d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f75345f-ca10-4511-9ef1-32169fa41884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories successfully set.\n"
     ]
    }
   ],
   "source": [
    "# Import user-defined modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "SRC_DIR = Path.cwd().parent / \"src\"\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "import data_utils\n",
    "\n",
    "# Set global variables\n",
    "RAW_DATA_DIR, PROCESSED_DATA_DIR = data_utils.get_data_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963934f-70b5-4406-a45f-90a5cca5c762",
   "metadata": {},
   "source": [
    "We listed the numeric, nominal, and ordinal categories along with its ordinal categories. This uses the dataset we cleaned during the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab83906-7ccb-4f4d-89b5-5f69d1e05fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "numeric_features = ['age', 'study_hours_per_day', 'attendance_percentage', \n",
    "                    'sleep_hours', 'exercise_frequency', 'mental_health_rating', \n",
    "                    'total_screen_time']\n",
    "\n",
    "nominal_features = ['gender', 'part_time_job', 'internet_quality', 'extracurricular_participation']\n",
    "\n",
    "ordinal_features = ['diet_quality', 'parental_education_level']\n",
    "ordinal_categories = [\n",
    "    ['Poor', 'Fair', 'Good'],  # diet_quality\n",
    "    ['No education', 'High School', 'Bachelor', 'Master']  # parental education\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17031ae0-a753-4e6c-be34-b82d40aead76",
   "metadata": {},
   "source": [
    "## Creating Pipelines and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fe5f6-dcd6-42a0-84ae-3ebd253da0b7",
   "metadata": {},
   "source": [
    "We used a standard scaler because we saw during the exploratory data analysis that our numeric features are in fact showing a normalized distribution. For nominal data, we used one-hot encoder because this is a very straightforward way of encoding categorical data. There are also few unique values in our nominal features so it's a good fit. We also used an ordinal encoder for the ordinal features which are diet quality and parental education level. This is important because it outputs an increasing value for higher order data. As you can see, we also didn't use any imputing technique, because we have already cleaned the data and handled the imputing earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b15017-5f32-44ed-8d84-cee4e10f6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Nominal pipeline\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Ordinal pipeline\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "])\n",
    "\n",
    "# Combine all\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('nom', nominal_transformer, nominal_features),\n",
    "    ('ord', ordinal_transformer, ordinal_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef073de-1204-4c15-b86d-55360345e57f",
   "metadata": {},
   "source": [
    "We used this `ColumnTransformer` pipeline to preprocess, encode, and scale the data from the cleaned dataset. We removed the exam score field because this is not a feature but a label that we're trying to predict in the future training runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fff4ba-eae7-45f4-a8c3-e4f24793603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.read_csv(f\"{PROCESSED_DATA_DIR}/cleaned_dataset.csv\")\n",
    "\n",
    "X = cleaned_df.drop(columns=['exam_score'])\n",
    "\n",
    "# Fit + transform\n",
    "X_preprocessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10dedd-7a2f-4315-9947-f8fcb5be66e1",
   "metadata": {},
   "source": [
    "We then saved this preprocessed dataset as `scaled_encoded_global_dataset.csv` for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de870481-a7d0-4715-9438-a28f971776a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories successfully set.\n",
      "Dataset saved in /home/asimov/Projects/lifestyle_learning/data/preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)\n",
    "\n",
    "# Save to CSV\n",
    "data_utils.save_dataset_to_csv(X_preprocessed_df, \"scaled_encoded_global_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bdfc0-0eac-480f-92c3-20f7c0097be0",
   "metadata": {},
   "source": [
    "A copy of the transformer is present in `data_utils` just to avoid reprogramming of the logic as the pipeline works for all dataset clusters derived from the original cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b89e3541-92d0-4952-98fa-c3b39daf1fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], shape=(1000, 19))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see here that data_utils.preprocessor can be used to perform the same functionality\n",
    "# of the ColumnTransformer above\n",
    "X_preprocessed == data_utils.preprocessor(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (teece2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
